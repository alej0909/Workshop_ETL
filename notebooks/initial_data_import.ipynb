{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 **Notebook: Initial Data Import**\n",
    "Welcome to this first notebook. In this notebook, we will focus on connecting to the database and creating the necessary tables. Specifically, we will connect locally to a PostgreSQL instance. Afterward, we will perform some brief transformations on our dataset, which is initially in a CSV format, to prepare it for insertion into the tables we created earlier.\n",
    "\n",
    "Before proceeding, ensure that you have already installed the necessary dependencies listed in the requirements.txt file. You can do this by running the following command:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 🗂️ Set Workdir\n",
    "Ensure that you already have your own .env file containing your environment variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "work_dir = os.getenv('WORK_DIR')\n",
    "\n",
    "# Ensure the working directory is in sys.path\n",
    "sys.path.append(work_dir)\n",
    "\n",
    "from src.db_connection import build_engine\n",
    "from src.transform_data import DataTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 📝 **Explanation:**\n",
    "\n",
    "- We load environment variables to access database credentials securely.\n",
    "- The working directory is added to `sys.path` to ensure our project modules are easily accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 🔌 **Connect to the Database & 📦 Import Libraries**\n",
    "\n",
    "With our environment set up, we'll now connect to the PostgreSQL database using SQLAlchemy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import CandidatesRaw, CandidatesTransformed\n",
    "from src.db_connection import build_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from src.transform_data import DataTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the database postgres!\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "engine = build_engine()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 **Explanation:**\n",
    "\n",
    "- We use the `build_engine` function to establish a connection to the database. This connection will be used throughout the notebook to interact with the database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🗄️ **Create the Database Table**\n",
    "\n",
    "Here, we check if the CandidatesRaw table exists. If it does, it will be dropped and then recreated. This ensures that the table structure is always up to date. Note: Be cautious when running this in a production environment, as it will drop the existing table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save transformed data to a new CSV\n",
    "transformer.save_transformed_data('../data/candidates_transformed.csv')\n",
    "\n",
    "try:\n",
    "    if inspect(engine).has_table('CandidatesRaw'):\n",
    "        CandidatesRaw.__table__.drop(engine)\n",
    "    CandidatesRaw.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 **Explanation:**\n",
    "\n",
    "- **DataTransformer**: A custom class designed to handle our data transformations.\n",
    "- **Standardize Column Names**: We standardize column names to ensure consistency and avoid issues with naming conventions.\n",
    "- **Generate IDs**: Each candidate is assigned a unique ID for easier reference and database operations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📤 **Load Data into the Table**\n",
    "\n",
    "\n",
    "Now, we'll load the original dataset, which is already normalized, into the CandidatesRaw table. Since the dataset is normalized, we don't need to add any additional columns such as ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Inicializar la clase transformadora con el archivo CSV\n",
    "    transformer = DataTransformer('../data/candidates.csv')\n",
    "    \n",
    "    # Estandarizar los nombres de las columnas\n",
    "    transformer.standardize_column_names()\n",
    "\n",
    "    # Generate unique IDs for each candidate\n",
    "    transformer.generate_ids()\n",
    "    \n",
    "    # Subir los datos a la tabla 'CandidatesRaw'\n",
    "    transformer.data.to_sql('CandidatesRaw', con=engine, if_exists='append', index=False)\n",
    "    print(\"Data uploaded\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if hasattr(engine, 'dispose'):\n",
    "        engine.dispose()\n",
    "\n",
    "    if 'session' in locals():\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 **Explanation:**\n",
    "\n",
    "- **Save to CSV**: We save the transformed data to a new CSV file as a backup and for any future reference.\n",
    "- **Insert to Database**: The transformed data is then inserted into the `Candidates_raw` table in our PostgreSQL database.\n",
    "\n",
    "---\n",
    "# ✅ **Summary**\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "- Configured our environment and set up the necessary modules.\n",
    "- Established a connection to our PostgreSQL database.\n",
    "- Loaded raw candidate data, performed initial transformations, and saved it to the database.\n",
    "\n",
    "🎉 **Next Steps**: We'll clean the data further and perform exploratory data analysis (EDA) in the next notebook. \n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **Notebook Flow**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 🔧 | Configure the Environment |\n",
    "| 🔌 | Connect to the Database |\n",
    "| 📂 | Load and Transform Data |\n",
    "| 🗄️ | Save Raw Data to Database |\n",
    "\n",
    "### 🎯 **Objectives Met**\n",
    "\n",
    "- Environment configured and secured 🔐\n",
    "- Database connection established 🔗\n",
    "- Raw data loaded and transformed 📈\n",
    "- Data successfully saved to PostgreSQL 🗄️\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_workshop1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
